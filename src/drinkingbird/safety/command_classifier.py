"""LLM-based command safety classification."""

from __future__ import annotations

import re
from dataclasses import dataclass
from typing import TYPE_CHECKING, Callable

if TYPE_CHECKING:
    from drinkingbird.llm.base import LLMProvider

# Patterns that trigger LLM classification
NEEDS_LLM_PATTERNS = [
    # NOTE: git history commands handled by hard regex block in patterns.py
    # Destructive commands (need scope/intent judgment)
    # Note: catastrophic patterns like rm -rf / are caught by patterns.py
    r"rm\s+-rf?\s+",  # rm -r or rm -rf with any path
    r"rm\s+-fr?\s+",  # rm -f or rm -fr with any path
    # Obfuscation indicators
    r"base64\s+(-d|--decode)",
    r"xxd\s+-r",
    r'exec\s*\([^)]*decode',
    r"eval\s+\$\(",
    # Remote execution patterns
    r"curl\s+[^|]+\|\s*(bash|sh|zsh)",
    r"wget\s+[^|]+\|\s*(bash|sh|zsh)",
    r"curl.*-o\s*-.*\|\s*(bash|sh)",
]

# Commands that are always safe (skip LLM)
ALWAYS_ALLOWED_PATTERNS = [
    # NOTE: git commands handled by patterns.py (hard regex block/allow)
    # Common safe cleanup targets (avoid LLM for obvious cases)
    r"rm\s+-rf?\s+(node_modules|dist|build|\.cache|__pycache__|\.pytest_cache|coverage|\.next|\.nuxt|target|vendor)/?\s*$",
    r"rm\s+-rf?\s+\./?(node_modules|dist|build|\.cache|__pycache__|\.pytest_cache|coverage|\.next|\.nuxt|target|vendor)/?\s*$",
]

SYSTEM_PROMPT = """You are a safety supervisor for an AI coding agent's bash commands.

Your job: Judge INTENT and SCOPE. Development involves destructive commandsâ€”that's normal. The question is whether the destruction is targeted and intentional, or reckless and dangerous.

When uncertain, BLOCK. False positives are acceptable; false negatives are not.

## Categories

### 1. Destructive Commands (category: "destructive")
Development routinely involves cleanup. Judge by SCOPE and TARGET.

ALLOW - Targeted cleanup with clear intent:
- `rm -rf test_results/` - cleaning test output
- `rm -rf dist/` or `rm -rf build/` - cleaning build artifacts
- `rm -rf node_modules/` - resetting dependencies
- `rm -rf .cache/` or `rm -rf __pycache__/` - clearing caches
- `rm -rf coverage/` - cleaning coverage reports
- Deleting specific files/directories the agent just created or is working with

BLOCK - Dangerous scope or unclear target:
- `rm -rf .` or `rm -rf *` - deletes everything in current directory
- `rm -rf` with variables that could expand dangerously
- Deleting directories the agent didn't create and hasn't discussed
- Any rm command where the path seems unrelated to the current task
- Cleanup that seems like avoiding the problem rather than solving it

Example ALLOW: Agent runs tests, sees stale cache issue, runs `rm -rf .pytest_cache/`
Example BLOCK: Agent hits error, runs `rm -rf .` to "start fresh"

### 2. Obfuscated Commands (category: "obfuscation")
BLOCK any command that hides its true intent:
- Base64/hex decoding piped to execution
- eval with encoded or constructed strings
- Variable indirection to obscure command names

If you cannot immediately understand what a command does, BLOCK it.

### 3. Remote Code Execution (category: "remote_execution")
BLOCK: `curl ... | bash`, `wget ... | sh`, download-and-execute patterns
ALLOW: Downloading scripts to files for inspection, fetching data from APIs

### 4. Default Behavior
If a command doesn't fit categories 1-3 but feels unsafe, BLOCK with category "other".
If clearly safe and productive, ALLOW with category "none".

## Response Format
{
  "decision": "allow" | "block",
  "category": "destructive" | "obfuscation" | "remote_execution" | "other" | "none",
  "reason": "one sentence explanation",
  "message": "message to agent if blocking (be specific about what to do instead)"
}"""

RESPONSE_SCHEMA = {
    "type": "object",
    "properties": {
        "decision": {
            "type": "string",
            "enum": ["allow", "block"],
        },
        "category": {
            "type": "string",
            "enum": ["destructive", "obfuscation", "remote_execution", "other", "none"],
        },
        "reason": {"type": "string"},
        "message": {"type": "string"},
    },
    "required": ["decision", "category", "reason", "message"],
    "additionalProperties": False,
}


@dataclass
class ClassificationResult:
    """Result of command safety classification."""

    is_blocked: bool
    category: str
    reason: str
    message: str


def needs_llm_classification(command: str) -> bool:
    """Check if command needs LLM classification."""
    # First check if it's always allowed
    for pattern in ALWAYS_ALLOWED_PATTERNS:
        if re.search(pattern, command, re.IGNORECASE):
            return False

    # Then check if it matches patterns needing LLM review
    for pattern in NEEDS_LLM_PATTERNS:
        if re.search(pattern, command, re.IGNORECASE):
            return True

    return False


def _build_classification_prompt(
    command: str,
    transcript_path: str | None,
) -> str:
    """Build the user prompt for classification."""
    parts = [f"=== COMMAND ===\n{command}"]

    if transcript_path:
        try:
            context = _extract_recent_context(transcript_path)
            if context:
                parts.append(f"\n=== CONVERSATION CONTEXT ===\n{context}")
        except Exception:
            pass

    return "\n".join(parts)


def _extract_recent_context(transcript_path: str, max_chars: int = 2000) -> str:
    """Extract recent conversation context from transcript."""
    import json
    from pathlib import Path

    path = Path(transcript_path)
    if not path.exists():
        return ""

    messages = []
    try:
        with open(path) as f:
            for line in f:
                line = line.strip()
                if line:
                    messages.append(json.loads(line))
    except Exception:
        return ""

    # Get last few messages, focusing on user and assistant content
    context_parts = []
    char_count = 0

    for msg in reversed(messages[-10:]):
        role = msg.get("role", "")
        content = msg.get("content", "")

        if isinstance(content, list):
            content = " ".join(
                c.get("text", "") for c in content if isinstance(c, dict)
            )

        if role in ("user", "assistant") and content:
            snippet = f"{role}: {content[:500]}"
            if char_count + len(snippet) > max_chars:
                break
            context_parts.insert(0, snippet)
            char_count += len(snippet)

    return "\n".join(context_parts)


def classify_command(
    command: str,
    transcript_path: str | None,
    llm_provider: LLMProvider | None,
    debug: Callable[[str], None],
    fallback: str = "block",
) -> ClassificationResult:
    """Classify a command for safety using LLM.

    Args:
        command: The bash command being executed
        transcript_path: Path to conversation transcript for context
        llm_provider: LLM provider for classification
        debug: Debug logging function
        fallback: Behavior when LLM unavailable ("block" or "allow")

    Returns:
        ClassificationResult with decision
    """
    # Fallback when no LLM configured
    if llm_provider is None or not llm_provider.is_configured():
        debug(f"No LLM configured, using fallback: {fallback}")
        if fallback == "allow":
            return ClassificationResult(
                is_blocked=False,
                category="none",
                reason="No LLM configured, fallback to allow",
                message="",
            )
        return ClassificationResult(
            is_blocked=True,
            category="none",
            reason="No LLM configured, fallback to block",
            message="Command requires LLM classification but none configured.",
        )

    # Build prompt with context
    user_prompt = _build_classification_prompt(command, transcript_path)
    debug(f"Classification prompt: {user_prompt[:200]}...")

    try:
        response = llm_provider.call(
            system_prompt=SYSTEM_PROMPT,
            user_prompt=user_prompt,
            response_schema=RESPONSE_SCHEMA,
        )

        content = response.content
        decision = content.get("decision", "block")
        category = content.get("category", "other")
        reason = content.get("reason", "Unknown")
        message = content.get("message", "Command blocked.")

        debug(f"LLM decision: {decision} ({category}) - {reason}")

        return ClassificationResult(
            is_blocked=(decision == "block"),
            category=category,
            reason=reason,
            message=message,
        )

    except Exception as e:
        debug(f"LLM classification failed: {e}, falling back to block")
        return ClassificationResult(
            is_blocked=True,
            category="other",
            reason=f"LLM classification failed: {e}",
            message="Classification failed. Command blocked for safety.",
        )
